# Ubuntu安装spark步骤

---
> ## Contact me:
> Blog -> <https://cugtyt.github.io/blog/index>  
> Email -> <cugtyt@qq.com>, <cugtyt@gmail.com>  
> GitHub -> [Cugtyt@GitHub](https://github.com/Cugtyt)

---

**只安装spark和pyspark，scala以及hadoop不涉及。**

1. 安装java，如果已经安装跳过。

``` bash
$ sudo apt install openjdk-8-jdk
```
注意写博客时安装jdk9仍然存在问题，因此安装jdk8.

``` bash
$ java -version
```
输出版本信息表示安装成功

2. 下载spark文件，在官网下载文件，例如最新版本为：spark-2.2.0-bin-hadoop2.7.tar

解压：
``` bash
$ tar -xvf spark-2.2.0-bin-hadoop2.7.tar
```

将解压后的文件夹移动到合适的目录，例如/home/username/spark，那么现在的路径就是：/home/cugtyt/software/spark/spark-2.2.0-bin-hadoop2.7，也可以为了方便改名字。

3. 修改/etc/profile

在最后加入：

``` bash
export SPARK_HOME=/home/cugtyt/software/spark/spark-2.2.0-bin-hadoop2.7
export PATH=$PATH:${SPARK_HOME}/bin
export PYSPARK_PYTHON=python3
```

最后一行是为了使用python3，如果使用python2，删去它。

运行：
``` bash
$ source /etc/profile
```

4. 安装pyspark

``` bash
$ sudo pip3 install pyspark
$ pyspark
```

正常情况下就可以运行了。