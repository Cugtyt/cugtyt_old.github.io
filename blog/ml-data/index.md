# **Blogs of *Machine Learning & Data***

## Contact me

* Blog -> <https://cugtyt.github.io/blog/index>
* Email -> <cugtyt@qq.com>, <cugtyt@gmail.com>
* GitHub -> [Cugtyt@GitHub](https://github.com/Cugtyt)

---

本系列博客主要是关于机器学习和数据科学相关内容。

---

## [**run-length 编码和解码**](https://cugtyt.github.io/blog/ml-data/2018/082211)

> 通过记录连续1的起始位置和长度来编码，并根据这个信息解码

---

## [**Michael Nielsen对梯度消失的解释（二）**](https://cugtyt.github.io/blog/ml-data/2018/201802101819)

> 梯度消失的数学解释以及梯度爆炸，梯度不稳定问题

---

## [**Michael Nielsen对梯度消失的解释（一）**](https://cugtyt.github.io/blog/ml-data/2018/201802101619)

> 为什么会出现梯度消失

---

## [**Michael Nielsen对softmax的解释**](https://cugtyt.github.io/blog/ml-data/2018/201802101439)

> softmax+对数似然损失同样解决了学习缓慢的问题

---

## [**Michael Nielsen对交叉熵的解释（三）**](https://cugtyt.github.io/blog/ml-data/2018/201802092159)

> 与二次损失函数的对比和结果讨论

---

## [**Michael Nielsen对交叉熵的解释（二）**](https://cugtyt.github.io/blog/ml-data/2018/201802092106)

> 数学表达式上交叉熵避免了学习变慢的问题

---

## [**Michael Nielsen对交叉熵的解释（一）**](https://cugtyt.github.io/blog/ml-data/2018/201802092000)

> 二次损失函数+sigmoid会出现学习变慢

---

## [**Michael Nielsen对weight初始化的解释**](https://cugtyt.github.io/blog/ml-data/2018/201802091801)

> 标准化高斯随机初始化导致饱和问题
> 使用Xavier初始化加速训练

---