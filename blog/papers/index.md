# **Blogs of Papers**

## Contact me

* Blog -> <https://cugtyt.github.io/blog/index>
* Email -> <cugtyt@qq.com>, <cugtyt@gmail.com>
* GitHub -> [Cugtyt@GitHub](https://github.com/Cugtyt)

---

本系列博客主要是关于阅读论文的笔记。

---

## [**[Cycle GAN] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks**](https://cugtyt.github.io/blog/papers/2018/0811)

> 在无数据对的情况下学习转换，学习x->y->x，损失有对抗损失和环形一致损失

---

## [**[CoordConv] An intriguing failing of convolutional neural networks and the CoordConv solution**](https://cugtyt.github.io/blog/papers/2018/0809)

> 相比普通卷积加入了坐标层，在一些问题上比普通卷积好很多

---

## [**Mask R-CNN**](https://cugtyt.github.io/blog/papers/2018/0808)

> 相比Faster R-CNN加入了一个掩模分支用于生成掩模，在其他任务上表现不错

---

## [**Faster R-CNN**](https://cugtyt.github.io/blog/papers/2018/0807)

> 对区域提议使用网络生成，这个网络和目标检测网络共享特征，速度更快，效果更好

---

## [**Fast R-CNN**](https://cugtyt.github.io/blog/papers/2018/0805)

> R-CNN and SPPnet缺陷，一步到位的训练，使用多任务损失，RoI pooling layer，Truncated SVD

---

## [**[R-CNN] Rich feature hierarchies for accurate object detection and semantic segmentation**](https://cugtyt.github.io/blog/papers/2018/0803)

> 自底向上区域提议+CNN

---

## [**YOLOv3: An Incremental Improvement**](https://cugtyt.github.io/blog/papers/2018/0802)

> 对网络结构做了一些变动

---

## [**YOLO9000: Better, Faster, Stronger**](https://cugtyt.github.io/blog/papers/2018/0731)

> 使用一些方法提升了YOLO的性能，使用聚类设置框维度，使用分类数据和检测数据联合训练

---

## [**You Only Look Once: Unified, Real-Time Object Detection**](https://cugtyt.github.io/blog/papers/2018/0728)

> YOLO只使用一个神经网络，用回归问题解决目标检测，速度快，相比R-CNN准确率还差点

---

## [**Wasserstein GAN**](https://cugtyt.github.io/blog/papers/2018/0727)

> EM与其他距离和散度的比较，WGAN可以提升稳定性，更好的收敛

---

## [**Are GANs Created Equal? A Large-Scale Study**](https://cugtyt.github.io/blog/papers/2018/0725)

> fair and comprehensive comparison of the state-of-the-art GANs, it is necessary to report a summary of distribution of results, a series of tasks of increasing difficulty for which undisputed measures.  
> We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes.

---

## [**The GAN Landscape: Losses, Architectures, Regularization, and Normalization**](https://cugtyt.github.io/blog/papers/2018/0723)

> losses, regularization and normalization schemes, and neural architectures, and their impact on the on the quality of generated samples which we assess by recently introduced quantitative metrics.

---

## [**MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications**](https://cugtyt.github.io/blog/papers/2018/0721)

> Depthwise Separable Convolution, Width Multiplier: Thinner Models, Resolution Multiplier: Reduced Representation

---