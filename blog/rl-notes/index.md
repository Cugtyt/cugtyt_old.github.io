# **Blogs of *Reinforcement Learning***

## Contact me

* Blog -> <https://cugtyt.github.io/blog/index>
* Email -> <cugtyt@qq.com>, <cugtyt@gmail.com>
* GitHub -> [Cugtyt@GitHub](https://github.com/Cugtyt)

---

本系列博客主要是关于强化学习笔记。

主要内容来源于：[Thomas Simonini *Deep Reinforcement Learning Course with Tensorflow*](https://github.com/simoninithomas/Deep_reinforcement_learning_Course) 和 [Arthur Juliani *Simple Reinforcement Learning with Tensorflow series*](https://github.com/awjuliani/DeepRL-Agents)

---

## [***Thomas Simonini* Part 5: An intro to Advantage Actor Critic methods: let’s play Sonic the Hedgehog**](https://cugtyt.github.io/blog/rl-notes/201808011642)

> The problem with Policy Gradients, How Actor Critic works, A2C and A3C, A2C in practice

---

## [***Thomas Simonini* Part 4: An introduction to Policy Gradients with Cartpole and Doom**](https://cugtyt.github.io/blog/rl-notes/201807311544)

> Two types of policy, Advantages, Disadvantages, Policy Search, Monte Carlo Policy Gradients

---

## [***Thomas Simonini* Part 3+: Improvements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed Q-targets**](https://cugtyt.github.io/blog/rl-notes/201807201658)

> Fixed Q-targets, Double DQNs, Dueling DQN, Prioritized Experience Replay (PER), Doom Deathmatch agent

---

## [***Thomas Simonini* Part 3: An introduction to Deep Q-Learning: let’s play Doom**](https://cugtyt.github.io/blog/rl-notes/201807201622)

> Preprocessing part, The problem of temporal limitation, Experience Replay

---

## [***Thomas Simonini* Part 2: Diving deeper into Reinforcement Learning with Q-Learning**](https://cugtyt.github.io/blog/rl-notes/201807201554)

> Q-learning algorithm: learning the Action Value Function, The Q-learning algorithm Process, Q* Learning with FrozenLake

---

## [***Thomas Simonini* Part 1: An introduction to Reinforcement Learning**](https://cugtyt.github.io/blog/rl-notes/201807201508)

> Reinforcement Learning Process, Reward Hypothesis, Episodic or Continuing tasks, Monte Carlo vs TD Learning methods, Exploration/Exploitation trade off, Three approaches to Reinforcement Learning

---

## [***Arthur Juliani* Part 8 - Asynchronous Actor-Critic Agents (A3C)**](https://cugtyt.github.io/blog/rl-notes/201807241747)

> The 3 As of A3C, Implementing the Algorithm

---

## [***Arthur Juliani* Part 7 - Action-Selection Strategies for Exploration**](https://cugtyt.github.io/blog/rl-notes/201807241704)

> Greedy Approach, Random Approach, ϵ-Greedy Approach, Boltzmann Approach, Bayesian Approaches (w/ Dropout)

---

## [***Arthur Juliani* Part 6 - Partial Observability and Deep Recurrent Q-Networks**](https://cugtyt.github.io/blog/rl-notes/201807231515)

> limited, changing world, Recurrent Neural Networks, Implementing in Tensorflow

---

## [***Arthur Juliani* Part 4 - Deep Q-Networks and Beyond**](https://cugtyt.github.io/blog/rl-notes/201807201324)

> Double DQN, Dueling DQN

---

## [***Arthur Juliani* Part 3 - Model-Based RL**](https://cugtyt.github.io/blog/rl-notes/201807201233)

> Model-Based

---

## [***Arthur Juliani* Part 2 - Policy-based Agents**](https://cugtyt.github.io/blog/rl-notes/201807201126)

> Full reinforcement agent, Markov Decision Process, Cart-Pole Task

---

## [***Arthur Juliani* Part 1.5 - Contextual Bandits**](https://cugtyt.github.io/blog/rl-notes/201807201055)

> Multi-armed bandit, Contextual bandit, Full RL problem, Contextual bandit代码

---

## [***Arthur Juliani* Part 1 - Two-armed Bandit**](https://cugtyt.github.io/blog/rl-notes/201807201027)

> RL问题, Learning a *Policy*, Policy Gradients, Value functions, e-greedy policy, policy loss equation, The Multi-armed bandit 代码

---

## [***Arthur Juliani* Part 0 - Q-Learning Agents**](https://cugtyt.github.io/blog/rl-notes/201807201023)

> Policy Gradient, Q-Learning, Bellman Equation, Q-Table 代码, Q-Learning NN 代码

---