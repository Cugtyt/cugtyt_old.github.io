# Supervisor is all we need

## Contact me

* Blog -> <https://cugtyt.github.io/blog/index>
* Email -> <cugtyt@qq.com>
* GitHub -> [Cugtyt@GitHub](https://github.com/Cugtyt)

---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

## Supervisor is essential

No matter supervised learning, unsupervised learning, reinforcement learning, what we actually do is find a good supervisor. It is believed that data is the main difference in these three areas, but essentially it is supervisor, explicitly or inexplicitly. 

**Supervised learning** Data is paired $(x, y)$, and we can easily design object function to learn a model. From my perspective, we designed a supervisor, which is the object function, that evaluates the projection result $x \to y$. The task of model is to find the specific projection to satisfy the supervisor.

**Unsupervised learning** Data is only $x$, we want to discover its distribution or patten, the latent $y$ . The model we learnt is judged by hand designed metrics, which are supervisors. The unsupervised learning task is more difficult compared with supervised learning task since the supervisor is more harder to design or select.

**Reinforcement learning** Data is sequential and dynamic $(x_1, \dots, x_n, y)$, here $y$ is the reward, and $x$s is the interaction state-action trajectory. The goal is to maximize $y$ by changing the state-action trajectory. The supervisor lies in the middle reward, which means how good is current state (and with action) judged from the final reward.

## Supervisor determines learner's behavior

**Supervised learning** Bad object function results in bad model, it is because the poor supervisor evaluates the projection $x \to y$. The learning strategies, such as back propagation, SGD, evolution, are omitted, we will assume the learner is capable to solve the task we discussed. The model learning from data is mislead by the poor supervisor, and finally a model is learnt with poor performance. That is why we choose different loss function for different purpose, such as L1, L2, Lp, entropy and so on. Also, bad data is one reason for poor supervisor for it mislead the $x \to y$ projection and thus object function.

**Unsupervised learning** Prior knowledge and assumption are always needed in unsupervised learning, different assumption and metrics guide different learner behavior. And this flexibility increase the difficulty.

**Reinforcement learning** There are lots of strategy to calculate the middle reward, but the optimal is still not reached we believe. The explicitly supervisor is critic in Actor-Critic method, a specific model is designed to generate middle reward and performances better. Similarly, the discriminator in Generative Adversarial Network plays the same role, and generator will gain more if better discriminator is given.